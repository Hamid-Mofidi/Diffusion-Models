{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUhOKWTnpfZmLb85XUls2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamid-Mofidi/Diffusion-Models/blob/main/DDPM_Paper%20/DDPM_coding_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1hl8w4ajHuW"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import urllib\n",
        "import numpy as np\n",
        "import PIL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "SDZ5XworjI6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_image()-> PIL.Image.Image:\n",
        "    url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTZmJy3aSZ1Ix573d2MlJXQowLCLQyIUsPdniOJ7rBsgG4XJb04g9ZFA9MhxYvckeKkVmo&usqp=CAU'\n",
        "    filename = 'racoon.jpg'\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    return PIL.Image.open(filename)"
      ],
      "metadata": {
        "id": "tDuV8lAQjNVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_noise_distribution(noise, predicted_noise):\n",
        "    plt.hist(noise.cpu().numpy().flatten(), density = True, alpha = 0.8, label = \"ground truth noise\")\n",
        "    plt.hist(predicted_noise.cpu().numpy().flatten(), density = True, alpha = 0.8, label = \"predicted noise\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OqLTcHR-jQRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_noise_prediction(noise, predicted_noise):\n",
        "    plt.figure(figsize=(15,15))\n",
        "    f, ax = plt.subplots(1, 2, figsize = (5,5))\n",
        "    ax[0].imshow(reverse_transform(noise))\n",
        "    ax[0].set_title(f\"ground truth noise\", fontsize = 10)\n",
        "    ax[1].imshow(reverse_transform(predicted_noise))\n",
        "    ax[1].set_title(f\"predicted noise\", fontsize = 10)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Q5A4xDrVjSuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel:\n",
        "    def __init__(self, start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n",
        "        self.start_schedule = start_schedule\n",
        "        self.end_schedule = end_schedule\n",
        "        self.timesteps = timesteps\n",
        "        \n",
        "        \"\"\"\n",
        "        if \n",
        "            betas = [0.1, 0.2, 0.3, ...]\n",
        "        then\n",
        "            alphas = [0.9, 0.8, 0.7, ...]\n",
        "            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n",
        "            \n",
        "        \n",
        "        \"\"\" \n",
        "        self.betas = torch.linspace(start_schedule, end_schedule, timesteps)\n",
        "        self.alphas = 1 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        \n",
        "    def forward(self, x_0, t, device):\n",
        "        \"\"\"\n",
        "        x_0: (B, C, H, W)\n",
        "        t: (B,)\n",
        "        \"\"\"\n",
        "        noise = torch.randn_like(x_0)\n",
        "        sqrt_alphas_cumprod_t = self.get_index_from_list(self.alphas_cumprod.sqrt(), t, x_0.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t, x_0.shape)\n",
        "\n",
        "        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n",
        "        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n",
        "        \n",
        "        return mean + variance, noise.to(device)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def backward(self, x, t, model, **kwargs):\n",
        "        \"\"\"\n",
        "        Calls the model to predict the noise in the image and returns \n",
        "        the denoised image. \n",
        "        Applies noise to this image, if we are not in the last step yet.\n",
        "        \"\"\"\n",
        "        betas_t = self.get_index_from_list(self.betas, t, x.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t, x.shape)\n",
        "        sqrt_recip_alphas_t = self.get_index_from_list(torch.sqrt(1.0 / self.alphas), t, x.shape)\n",
        "        mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t, **kwargs) / sqrt_one_minus_alphas_cumprod_t)\n",
        "        posterior_variance_t = betas_t\n",
        "\n",
        "        if t == 0:\n",
        "            return mean\n",
        "        else:\n",
        "            noise = torch.randn_like(x)\n",
        "            variance = torch.sqrt(posterior_variance_t) * noise \n",
        "            return mean + variance\n",
        "    \n",
        "\n",
        "    @staticmethod\n",
        "    def get_index_from_list(values, t, x_shape):\n",
        "        batch_size = t.shape[0]\n",
        "        \"\"\"\n",
        "        pick the values from vals\n",
        "        according to the indices stored in `t`\n",
        "        \"\"\"\n",
        "        result = values.gather(-1, t.cpu())\n",
        "        \"\"\"\n",
        "        if \n",
        "        x_shape = (5, 3, 64, 64)\n",
        "            -> len(x_shape) = 4\n",
        "            -> len(x_shape) - 1 = 3\n",
        "            \n",
        "        and thus we reshape `out` to dims\n",
        "        (batch_size, 1, 1, 1)\n",
        "        \n",
        "        \"\"\"\n",
        "        return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n"
      ],
      "metadata": {
        "id": "lku02WZKjVGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (32, 32)"
      ],
      "metadata": {
        "id": "R-nlLfgmjw6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SHAPE), # Resize the input image\n",
        "    transforms.ToTensor(), # Convert to torch tensor (scales data into [0,1])\n",
        "    transforms.Lambda(lambda t: (t * 2) - 1), # Scale data between [-1, 1] \n",
        "])\n",
        "\n",
        "\n",
        "reverse_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda t: (t + 1) / 2), # Scale data between [0,1]\n",
        "    transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "    transforms.Lambda(lambda t: t * 255.), # Scale data between [0.,255.]\n",
        "    transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)), # Convert into an uint8 numpy array\n",
        "    transforms.ToPILImage(), # Convert to PIL image\n",
        "])"
      ],
      "metadata": {
        "id": "-GagEPIPjxiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pil_image = get_sample_image()\n",
        "torch_image = transform(pil_image)"
      ],
      "metadata": {
        "id": "htfSb7vCj8CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffusion_model = DiffusionModel()"
      ],
      "metadata": {
        "id": "cI-T53X1kA-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NO_DISPLAY_IMAGES = 5\n",
        "torch_image_batch = torch.stack([torch_image] * NO_DISPLAY_IMAGES)\n",
        "t = torch.linspace(0, diffusion_model.timesteps - 1, NO_DISPLAY_IMAGES).long()\n",
        "noisy_image_batch, _ = diffusion_model.forward(torch_image_batch, t, device)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "f, ax = plt.subplots(1, NO_DISPLAY_IMAGES, figsize = (100,100))\n",
        "\n",
        "for idx, image in enumerate(noisy_image_batch):\n",
        "    ax[idx].imshow(reverse_transform(image))\n",
        "    ax[idx].set_title(f\"Iteration: {t[idx].item()}\", fontsize = 100)\n",
        "plt.show()   "
      ],
      "metadata": {
        "id": "AlvnaqaSieql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "gfLVrF1siiao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "opVJTOf2ijEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out, time_embedding_dims, labels, num_filters = 3, downsample=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.time_embedding_dims = time_embedding_dims\n",
        "        self.time_embedding = SinusoidalPositionEmbeddings(time_embedding_dims)\n",
        "        self.labels = labels\n",
        "        if labels:\n",
        "            self.label_mlp = nn.Linear(1, channels_out)\n",
        "        \n",
        "        self.downsample = downsample\n",
        "        \n",
        "        if downsample:\n",
        "            self.conv1 = nn.Conv2d(channels_in, channels_out, num_filters, padding=1)\n",
        "            self.final = nn.Conv2d(channels_out, channels_out, 4, 2, 1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(2 * channels_in, channels_out, num_filters, padding=1)\n",
        "            self.final = nn.ConvTranspose2d(channels_out, channels_out, 4, 2, 1)\n",
        "            \n",
        "        self.bnorm1 = nn.BatchNorm2d(channels_out)\n",
        "        self.bnorm2 = nn.BatchNorm2d(channels_out)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, padding=1)\n",
        "        self.time_mlp = nn.Linear(time_embedding_dims, channels_out)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t, **kwargs):\n",
        "        o = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        o_time = self.relu(self.time_mlp(self.time_embedding(t)))\n",
        "        o = o + o_time[(..., ) + (None, ) * 2]\n",
        "        if self.labels:\n",
        "            label = kwargs.get('labels')\n",
        "            o_label = self.relu(self.label_mlp(label))\n",
        "            o = o + o_label[(..., ) + (None, ) * 2]\n",
        "            \n",
        "        o = self.bnorm2(self.relu(self.conv2(o)))\n",
        "\n",
        "        return self.final(o)    \n"
      ],
      "metadata": {
        "id": "xbASVdPAil5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, img_channels = 3, time_embedding_dims = 128, labels = False, sequence_channels = (64, 128, 256, 512, 1024)):\n",
        "        super().__init__()\n",
        "        self.time_embedding_dims = time_embedding_dims\n",
        "        sequence_channels_rev = reversed(sequence_channels)\n",
        "        \n",
        "        self.downsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels) for channels_in, channels_out in zip(sequence_channels, sequence_channels[1:])])\n",
        "        self.upsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels,downsample=False) for channels_in, channels_out in zip(sequence_channels[::-1], sequence_channels[::-1][1:])])\n",
        "        self.conv1 = nn.Conv2d(img_channels, sequence_channels[0], 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(sequence_channels[0], img_channels, 1)\n",
        "\n",
        "    \n",
        "    def forward(self, x, t, **kwargs):\n",
        "        residuals = []\n",
        "        o = self.conv1(x)\n",
        "        for ds in self.downsampling:\n",
        "            o = ds(o, t, **kwargs)\n",
        "            residuals.append(o)\n",
        "        for us, res in zip(self.upsampling, reversed(residuals)):\n",
        "            o = us(torch.cat((o, res), dim=1), t, **kwargs)\n",
        "            \n",
        "        return self.conv2(o)"
      ],
      "metadata": {
        "id": "oAT9Awo9iqf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NO_EPOCHS = 2000\n",
        "PRINT_FREQUENCY = 400\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = True\n",
        "\n",
        "unet = UNet(labels=False)\n",
        "unet.to(device)\n",
        "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "lE4I2VFOiwOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NO_EPOCHS):\n",
        "    mean_epoch_loss = []\n",
        "    \n",
        "    batch = torch.stack([torch_image] * BATCH_SIZE)\n",
        "    t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
        "\n",
        "    batch_noisy, noise = diffusion_model.forward(batch, t, device) \n",
        "    predicted_noise = unet(batch_noisy, t)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss = torch.nn.functional.mse_loss(noise, predicted_noise) \n",
        "    mean_epoch_loss.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % PRINT_FREQUENCY == 0:\n",
        "        print('---')\n",
        "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)}\")\n",
        "        if VERBOSE:\n",
        "            with torch.no_grad():\n",
        "                plot_noise_prediction(noise[0], predicted_noise[0])\n",
        "                plot_noise_distribution(noise, predicted_noise)"
      ],
      "metadata": {
        "id": "l5MAKPAti0K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    img = torch.randn((1, 3) + IMAGE_SHAPE).to(device)\n",
        "    for i in reversed(range(diffusion_model.timesteps)):\n",
        "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
        "        img = diffusion_model.backward(img, t, unet.eval())\n",
        "        if i % 50 == 0:\n",
        "            plt.figure(figsize=(2,2))\n",
        "            plt.imshow(reverse_transform(img[0]))\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "vMNmrQOKi3ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "NO_EPOCHS = 100\n",
        "PRINT_FREQUENCY = 10\n",
        "LR = 0.001\n",
        "VERBOSE = False\n",
        "\n",
        "unet = UNet(labels=True)\n",
        "unet.to(device)\n",
        "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "PsZKqMMSi8G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, drop_last=True)"
      ],
      "metadata": {
        "id": "QmlQNICMi-lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NO_EPOCHS):\n",
        "    mean_epoch_loss = []\n",
        "    mean_epoch_loss_val = []\n",
        "    for batch, label in trainloader:\n",
        "        t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
        "        batch = batch.to(device)\n",
        "        batch_noisy, noise = diffusion_model.forward(batch, t, device) \n",
        "        predicted_noise = unet(batch_noisy, t, labels = label.reshape(-1,1).float().to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.nn.functional.mse_loss(noise, predicted_noise) \n",
        "        mean_epoch_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    for batch, label in testloader:\n",
        "\n",
        "        t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        batch_noisy, noise = diffusion_model.forward(batch, t, device) \n",
        "        predicted_noise = unet(batch_noisy, t, labels = label.reshape(-1,1).float().to(device))\n",
        "\n",
        "        loss = torch.nn.functional.mse_loss(noise, predicted_noise) \n",
        "        mean_epoch_loss_val.append(loss.item())\n",
        "    if epoch % PRINT_FREQUENCY == 0:\n",
        "        print('---')\n",
        "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n",
        "        if VERBOSE:\n",
        "            with torch.no_grad():\n",
        "                plot_noise_prediction(noise[0], predicted_noise[0])\n",
        "                plot_noise_distribution(noise, predicted_noise)\n",
        "                \n",
        "        torch.save(unet.state_dict(), f\"epoch: {epoch}\")"
      ],
      "metadata": {
        "id": "0uE8lx95jBDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = UNet(labels=True)\n",
        "unet.load_state_dict(torch.load((\"epoch: 80\")))"
      ],
      "metadata": {
        "id": "6-wn4VFUjEYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "gS_4_RhGjFFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = len(classes)\n",
        "NUM_DISPLAY_IMAGES = 5"
      ],
      "metadata": {
        "id": "okNu3QcHjHbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(16)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "f, ax = plt.subplots(NUM_CLASSES, NUM_DISPLAY_IMAGES, figsize = (100,100))\n",
        "\n",
        "for c in range(NUM_CLASSES):\n",
        "    imgs = torch.randn((NUM_DISPLAY_IMAGES, 3) + IMAGE_SHAPE).to(device)\n",
        "    for i in reversed(range(diffusion_model.timesteps)):\n",
        "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
        "        labels = torch.tensor([c] * NUM_DISPLAY_IMAGES).resize(NUM_DISPLAY_IMAGES, 1).float().to(device)\n",
        "        imgs = diffusion_model.backward(x=imgs, t=t, model=unet.eval().to(device), labels = labels)\n",
        "    for idx, img in enumerate(imgs):\n",
        "        ax[c][idx].imshow(reverse_transform(img))\n",
        "        ax[c][idx].set_title(f\"Class: {classes[c]}\", fontsize = 100)\n",
        "        \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8PyzCuvXjJqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}